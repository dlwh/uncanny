\documentclass[11pt,leqno,twoside]{article}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{natbib}
\setlength{\parskip}{1.2ex}        % space between paragraphs
\setlength{\parindent}{2em}        % amount of indention
\setlength{\textwidth}{7truein}      % default = 6.5"
\setlength{\oddsidemargin}{-12mm}   % default = 0"
\setlength{\evensidemargin}{-12mm}   % default = 0"
\setlength{\textheight}{225mm}     % default = 9"
\setlength{\topmargin}{-12mm}      % default = 0"
\usepackage[all]{xy}
\usepackage{tipa}
\input xy
\xyoption{all}
\usepackage{listings}
\newcommand{\indicator}[1]{\mathbbm{1}{\left[ {#1} \right] }}
\lstdefinelanguage{Scala}%
{morekeywords={abstract,case,catch,char,class,%
def,else,extends,final,%
if,import,%
match,module,new,null,object,override,package,private,protected,%
public,return,super,this,throw,trait,try,val,var,with%
},%
sensitive,%
morecomment=[l]//,%
morecomment=[s]{/*}{*/},%
morestring=[b]``,%
morestring=[b]',%
showstringspaces=false%
}[keywords,comments,strings]%

\lstset{language=Scala,%
mathescape=true,%
columns=[c]fixed,%
basewidth={0.5em, 0.40em},%
basicstyle=\tt,%
keywordstyle=\bfseries,%
}

\title{HW2: Regression}
\author{David Hall \\ \texttt{dlwh@cs.berkeley.edu}}
\begin{document}
\maketitle

\section{Introduction}

This report investigates 

\section{Model and Optimization}

The model we use is a simple regularized regression model using either $\ell_2$ or $\ell_1$ loss and $\ell_2$ or $\ell_1$
regularization. That is, we seek to minimize, for $\{p,q\} in \{1,2\}$ and tuning parameter $\lambda$:
\begin{equation*}
  \begin{split}
    \min_\theta \sum_i |y^{(i)} - \theta^T x^{(i)}|^p + \lambda ||\theta||_q^q
   \end{split}
 \end{equation*}
When $p=q=2$, we recover ridge regression, and when $p=2 and q=1$
we recover the ``lasso,'' which can obtain sparse solutions. While
exact solutions are possible for most of some of these conditions,
we instead focus on gradient-based optimizations.

Specifically, we consider two algorithms. 



\section{Experimental Setup}

We used the 



\end{document}
